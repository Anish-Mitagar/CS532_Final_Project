{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/26 09:46:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.97187\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DiabetesPredictionPipeline\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"diabetes_prediction_dataset.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Columns for features and label\n",
    "numerical_features = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
    "categorical_features = ['gender', 'smoking_history']\n",
    "label_col = 'diabetes'\n",
    "\n",
    "# Preprocessing for Numerical Features\n",
    "num_assembler = VectorAssembler(inputCols=numerical_features, outputCol=\"num_features\")\n",
    "scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"scaled_num_features\")\n",
    "\n",
    "# Preprocessing for Categorical Features\n",
    "stages = []  # this will collect all the stages in our pipeline\n",
    "for categoricalCol in categorical_features:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Combine all processed numerical and categorical features into a single feature vector\n",
    "assembler_inputs = [c + \"classVec\" for c in categorical_features] + [\"scaled_num_features\"]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [num_assembler, scaler, assembler]\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol=label_col, featuresCol=\"features\")\n",
    "stages += [rf]\n",
    "\n",
    "# Create the pipeline with all stages\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10]) \\\n",
    "    .addGrid(rf.maxDepth, [5]) \\\n",
    "    .build()\n",
    "\n",
    "# CrossValidator for Model Selection\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\"),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)\n",
    "\n",
    "# Model Evaluation\n",
    "predictions = cvModel.transform(df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexer took 0.14988904 seconds\n",
      "OneHotEncoder took 0.01202375 seconds\n",
      "StringIndexer took 0.12544550 seconds\n",
      "OneHotEncoder took 0.01044146 seconds\n",
      "VectorAssembler took 0.00427175 seconds\n",
      "StandardScaler took 0.19891087 seconds\n",
      "VectorAssembler took 0.00539600 seconds\n",
      "CrossValidator took 9.33844817 seconds\n",
      "Test set accuracy = 0.97187\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DiabetesPredictionPipeline\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"diabetes_prediction_dataset.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Columns for features and label\n",
    "numerical_features = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
    "categorical_features = ['gender', 'smoking_history']\n",
    "label_col = 'diabetes'\n",
    "\n",
    "# Stages\n",
    "stages = []\n",
    "stage_times = []\n",
    "\n",
    "# Data Preprocessing for Numerical Features\n",
    "num_assembler = VectorAssembler(inputCols=numerical_features, outputCol=\"num_features\")\n",
    "scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"scaled_num_features\")\n",
    "\n",
    "# Data Preprocessing for Categorical Features\n",
    "for categoricalCol in categorical_features:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Combine all processed numerical and categorical features into a single feature vector\n",
    "assembler_inputs = [c + \"classVec\" for c in categorical_features] + [\"scaled_num_features\"]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [num_assembler, scaler, assembler]\n",
    "\n",
    "\n",
    "\n",
    "# Apply each stage manually and time them\n",
    "for stage in stages:\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # For estimators (like StringIndexer, OneHotEncoder, and StandardScaler), fit and then transform\n",
    "    if isinstance(stage, StringIndexer) or isinstance(stage, OneHotEncoder) or isinstance(stage, StandardScaler):\n",
    "        model = stage.fit(df)\n",
    "        df = model.transform(df)\n",
    "    else:  # For transformers (like VectorAssembler), just transform\n",
    "        df = stage.transform(df)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    stage_times.append((stage.__class__.__name__, end_time - start_time))\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol=label_col, featuresCol=\"features\")\n",
    "\n",
    "# Now apply CrossValidator\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10]) \\\n",
    "    .addGrid(rf.maxDepth, [5]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\"),\n",
    "                          numFolds=5)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "cvModel = crossval.fit(df)\n",
    "end_time = time.perf_counter()\n",
    "stage_times.append((\"CrossValidator\", end_time - start_time))\n",
    "\n",
    "# Print out the stage times\n",
    "for stage_name, timing in stage_times:\n",
    "    print(f\"{stage_name} took {timing:.8f} seconds\")\n",
    "\n",
    "# Model Evaluation (this is part of CrossValidator timing)\n",
    "predictions = cvModel.transform(df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "scale = os.getenv('SCALE', 0)\n",
    "scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
